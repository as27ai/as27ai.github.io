<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Who Captures the Profits in the GenAI Stack? | Aaryan Shah</title>
    <meta name="description" content="A concise tour of today‚Äôs GenAI stack‚Äîsilicon to apps‚Äîwhy NVIDIA currently captures the lion‚Äôs share of value, and how shifting stack boundaries will shape the next phase." />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="../assets/css/styles.css" />
  </head>
  <body class="blog-post">
    
    <header class="site-header">
      <div class="container header-content">
        <a href="../index.html" class="brand">Aaryan Shah</a>
        <div class="header-right">
        <nav class="site-nav" aria-label="Primary">
          <a href="../index.html#about">About</a>
          <a href="../index.html#blog">Blog</a>
          <a href="../index.html#contact">Contact</a>
          <button class="theme-toggle" type="button" aria-label="Toggle light and dark theme">üåó</button>
        </nav>
        </div>
      </div>
    </header>

    <main>
      <section class="hero" id="post-hero">
        <div class="container">
          <p class="eyebrow">Blog ¬∑ GenAI Strategy</p>
          <h1>Who can capture the most profits from the GenAI Stack today?</h1>
          <p class="lead">
            GenAI is no longer just an innovation - it‚Äôs the foundation for the next wave of tools, business models, and user experiences.
            In this fast shifting AI landscape, one core question prevails: who is best positioned to capture the most profits from this shift?
            OpenAI may have made ChatGPT a verb, but today, it's NVIDIA which stands out. Its dominant role in AI accelerators (mainly their GPUs),
            high margins, and expanding influence across the GenAI stack make it uniquely advantaged
          </p>
        </div>
      </section>

      <section class="about">
        <div class="container about-content">
          <article class="about-text" style="max-width: 900px;">
            <h2>1) The GenAI stack today</h2>
            <p>
              A four-layer view‚Äî<strong>Silicon/AI accelerators</strong>, <strong>Infrastructure/Cloud</strong>,
              <strong>Platform &amp; Models</strong>, and <strong>Applications</strong>‚Äîbest explains how capability turns into value.
              Most of the ongoing cost of an LLM API call sits in compute (GPUs/TPUs), not software OPEX. This puts outsized
              economics at the silicon layer.
            </p>

            <h2>2) How major players position across the stack</h2>
            <ul>
              <li><strong>Google</strong>: Vertically integrated (TPU ‚Üí GCP ‚Üí Gemini ‚Üí Workspace)</li>
              <li><strong>Meta</strong>: consumer reach + open source (Llama, MTIA chips)</li>
              <li><strong>AWS</strong>: infra-first with custom silicon (Trainium) to reduce NVIDIA dependency</li>
              <li><strong>Microsoft</strong>: breadth across stack (Azure, OpenAI partnership, Phi, Copilot)</li>
              <li><strong>NVIDIA</strong>: ‚Äúpicks &amp; shovels‚Äù provider dominating accelerators &amp; CUDA ecosystem</li>
            </ul>

            <h2>3) Why NVIDIA captures the most value (for now)</h2>
            <p>
              GenAI stack economics are not evenly distributed. Compute dominates the recurring costs, and NVIDIA sits at the
              heart of it. While cloud hyperscalers (i.e., AWS, Azure, GCP) and model developers (i.e., OpenAI, Antropic, Meta,
              etc.) battle for adoption and differentiation, NVIDIA‚Äôs role as the supplier of GenAI gold rush (akin to ‚Äúpicks &amp
              shovels‚Äù) allows it to profit from every stage of the boom - regardless which applications or models win.
            </p>
            <ul>
              <li><strong>Market dominance</strong>: NVIDIA controls over 80% of GPUs in AI training/deployment and powers
                more than 75% of world‚Äôs TOP500 supercomputers; sole supplier of cutting edge accelerators (e.g., H100s, B200s,
                GB200s) for most enterprise-scale AI workloads
              </li>
              <li><strong>Financial performance</strong>: FY25 revenue ~$131B (+114% YoY), ~88% from compute and networking; strong
                pricing power and demand elasticity desire high ASPs
              </li>
              <li><strong>Unmatched margins</strong>: State-of-the-art accelerators yield outsized gross margins due to technological lead
                and lack of substitutes; margins supported by multi-quarter lead times for flagship GPUs
              </li>
              <li><strong>Ecosystem and switching</strong>: Proprietary CUDA software stack entrenches developers - switching requires
                significant codebase overhaul; developer ecosystem locks hyperscalers and enterprises into NVIDIA compatible workflows
              </li>
            </ul>
            <p>
              While NVIDIA‚Äôs current position is dominant, sustainability is not guaranteed:
            </p>
              <ul>
                <li>
                  <strong>Model efficiency gains</strong>: in the future incremental capability gains may not require proportional
                  increase in GPU compute; new optimization techniques could reduce FLOPs per inference
                </li>
                <li>
                  <strong>Custom silicon competition</strong>: hyperscalers and chip focused start-ups (e.g., Samba Nova) are investing
                  capital to reduce NVIDIA dependency and capture cost efficiency
                </li>
                <li>
                  <strong>Data center cyclicality</strong>: : hyperscalers may pause capex cycles once initial capacity is filled; demand
                  partly driven by catch-up infra build and may normalize post 2026
                </li>
                <li>
                  <strong>Regulatory risks</strong>: export controls (e.g., China) and supply chain constraints
                </li>
              </ul>

            <p>
              NVIDIA controls the scarcest and most non-substitutable layer - compute accelerators. As the foundational model gains
              plateau and app margins tighten, its role in powering inference with increased demand (i.e., reasoning models,
              more workloads) ensures sustained profitability. Hyperscalers (AWS, GCP, Azure, OCI) are deploying GB200s globally,
              signaling sustained medium-term growth. Rising cash reserves enable investment in next-gen GPUs (e.g., Blackwell
              architecture), manufacturing capacity, and vertical expansion via DGX Cloud - cementing NVIDIA‚Äôs lead in silicon
              and boosting profit capture across the stack.
            </p>

            <h2>4) Blurring boundaries across layers</h2>
            <p>
              Winning in the GenAI era will require more than scale. It will require building/owning irreplaceable assets, reducing
              partner dependency, and evolving faster than the market. 
            </p>
            <p>
              Unlike previous shifts, GenAI‚Äôs stack boundaries are not fixed anymore as companies are moving fluidly between layers
              with a goal to capture more value, reduce dependency, and accelerate innovation and set industry standards.
              This is leading to unprecedented vertical integration and cross layer competition:
            </p>
            
            <ul>
              <li>
                <strong>Silicon ‚Üí Infra &amp; Platform</strong>
                <ul>
                  <li>
                    NVIDIA moving from chipmaker to service provider with <strong>DGX Cloud</strong> and <strong>CUDA</strong> frameworks
                    for model training touching infra and platform layers
                  </li>
                 <li>
                   Microsoft with <strong>Maia</strong> and AWS with <strong>Trainium</strong>expanding into silicon
                   from infra
                  </li> 
                </ul>
              </li>

              <li>
                <strong>Infra ‚Üí Platform &amp; Apps</strong>
                <ul>
                  <li>
                   Microsoft setting up <strong>Azure Foundry</strong> and merging infra and platform. Copilot pushes
                    into application territory with Azure as the infra backbone
                  </li>
                 <li>
                   AWS bridging infra and platform with <strong>Bedrock</strong> enabling full application deployment
                   environments
                  </li> 
                </ul>
              </li>

              <li>
                <strong>Platform ‚Üí Infra &amp; Apps</strong>
                <ul>
                  <li>
                    OpenAI running <strong>ChatGPT</strong> (application) and offering APIs (platform) hosted on Azure (infra)
                  </li>
                 <li>
                   <strong>Claude as an app and API</strong> ‚Äì integrating platform and application roles
                  </li> 
                </ul>
              </li>

              <li>
                <strong>Silicon &amp; Infra ‚Üí Platform &amp; Apps</strong>
                <ul>
                  <li>
                    Google using <strong>TPUs</strong> (silicon), <strong>GCP</strong> (infra + platform), and
                    <strong>Gemini</strong> integrated into Workspace (apps) to make it a <em>full-stack AI provider</em>
                  </li>
                 <li>
                   Meta historically being an app company, releasing open source <strong>LLaMA</strong> models (platform),
                   and now developing chips (MTIA) expanding into silicon
                  </li> 
                </ul>
              </li>              
            </ul>


            <h2>5) How winners will differentiate</h2>
            <p>
              As the GenAI stack matures, and boundaries blur - cloud providers building models, chipmakers launching cloud services,
              and model developers creating end-user apps. In this environment, winners will be those that intentionally build
              defensibility across layers. Three strategic levers that may drive differentiation are:
            </p>
            <ul>
              <li><strong>Capital for proprietary advantage</strong>: invest to secure unique, hard to replicate assets such as specialized hardware,
                proprietary data and models; assets that create lasting performance, cost, or capability advantage (e.g., NVIDIA‚Äôs CUDA)
              </li>
              <li><strong>Integration depth</strong>: embed GenAI directly into workflows and ecosystems increasing switching costs and ensuring customer
                lock-in (e.g., Copilot across Office 365, GitHub, VS Code)
              </li>
              <li><strong>Adaptive expansion</strong>: move quickly into emerging adjacencies and respond to shifting stack boundaries
                (e.g., OpenAI evolving from API provider to integrating a consumer app via ChatGPT)
              </li>
            </ul>
            <p>
              Winning in the GenAI era will require more than scale. It will require building/owning irreplaceable assets, reducing partner
              dependency, and evolving faster than the market.
            </p>

            <hr />
            <!-- <p><em>Originally developed from my internal write-up ‚ÄúGenAI stack v5.‚Äù</em></p> -->
            <p><a class="text-link" href="../index.html#blog">‚Üê Back to all posts</a></p>
          </article>
        </div>
      </section>
    </main>

    <footer class="site-footer" id="contact">
      <div class="container footer-content">
        <div>
          <h2>Let's stay in touch!</h2>
          <p>Reach out for collaborations, thought partnership, brainstorming ideas, conversations on AI, or a game of chess üòÑ</p>
        </div>
        <div class="footer-links">
          <a href="mailto:aaryanshah661@gmail.com">E-mail</a>
          <a href="https://www.linkedin.com/in/shahaaryan" target="_blank" rel="noopener noreferrer">LinkedIn</a>
         <!-- <a href="https://github.com/ashah661" target="_blank" rel="noopener noreferrer">GitHub</a>-->
        </div>
      </div>
      <p class="footer-copy">¬© <span id="current-year"></span> Aaryan Shah. All rights reserved.</p>
    </footer>

    <script>
      document.getElementById("current-year").textContent = new Date().getFullYear();
    </script>
    <script>
      (function () {
        const saved = localStorage.getItem('theme');
        if (saved === 'light') document.documentElement.setAttribute('data-theme', 'light');
      })();
      
      document.querySelector('.theme-toggle').addEventListener('click', () => {
        const html = document.documentElement;
        const isLight = html.getAttribute('data-theme') === 'light';
        if (isLight) {
          html.removeAttribute('data-theme');
          localStorage.removeItem('theme');
        } else {
          html.setAttribute('data-theme', 'light');
          localStorage.setItem('theme', 'light');
        }
      });
    </script>
  </body>
</html>
